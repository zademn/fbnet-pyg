{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19afa9b1-783c-454c-99fb-4cbee2afb151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "[Open3D INFO] Resetting default logger to print to terminal.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from typing import Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import torch\n",
    "import torch_geometric\n",
    "from einops import rearrange\n",
    "from open3d.web_visualizer import draw\n",
    "from torch import nn\n",
    "from torch_geometric.datasets import ModelNet, ShapeNet\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import MessagePassing, knn, knn_graph\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdeb2117-54de-4a73-b547-cfc699cb24db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04df5ed2-a99e-44f9-8c82-e8675f870ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_pcd_graph(points: np.array, edge_list: np.array):\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(points)\n",
    "    ls = o3d.geometry.LineSet.create_from_point_cloud_correspondences(\n",
    "        pcd, pcd, edge_list\n",
    "    )\n",
    "    ls.paint_uniform_color([0.5, 0.5, 0.5])\n",
    "    o3d.visualization.draw_geometries([pcd, ls])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9152266-e09b-482a-bb92-6518801bb024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(pcd):\n",
    "    assert pcd.ndim == 3\n",
    "    b, n, _ = pcd.shape\n",
    "    batch = torch.repeat_interleave(torch.arange(b), repeats=n).type(torch.long)\n",
    "    return batch.to(pcd.device)\n",
    "\n",
    "\n",
    "def fps_subsample(pcd, n_points: int, random_start: bool = False):\n",
    "    b, n, _ = pcd.shape\n",
    "    batch = get_batch(pcd)\n",
    "    pcd = rearrange(pcd, \"b n c -> (b n) c\")\n",
    "    idxs = fps(pcd, batch=batch, ratio=n_points / n, random_start=False)\n",
    "    return to_dense_batch(pcd[idxs], batch=batch[idxs])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee1c0c7-b5e3-4aa2-b754-74fcbae20dbe",
   "metadata": {},
   "source": [
    "# Cross transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c008531c-3b47-4ee1-863d-7aab9a69f57f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1 = torch.tensor([[0, 0, 0], [10, 10, 10], [20, 20, 20]]).type(torch.float32)\n",
    "p1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "474834a0-61ff-42b3-8757-1789532fd431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 3])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2 = torch.tensor(\n",
    "    [\n",
    "        [1, 1, 1],\n",
    "        [-1, -1, -1],\n",
    "        [11, 11, 11],\n",
    "        [9, 9, 9],\n",
    "        [21, 21, 21],\n",
    "        [19, 19, 19],\n",
    "    ]\n",
    ").type(torch.float32)\n",
    "p2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3eca1579-7fce-4e14-a4ce-e99b53c48ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "p3 = torch.rand(2048, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "93de6e93-fc5a-4b03-97b5-b2f42f767b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = torch.ones(3, 2)\n",
    "f2 = torch.ones(6, 2) * 2\n",
    "f3 = torch.ones(2048, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0bcb4aaf-7124-4886-97c9-89e95eb9b0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_ = rearrange(p1, \"n d -> 1 d n\").cuda()\n",
    "p2_ = rearrange(p2, \"n d -> 1 d n\").cuda()\n",
    "p3_ = rearrange(p3, \"n d -> 1 d n\").cuda()\n",
    "f1_ = rearrange(f1, \"n d -> 1 d n\").cuda()\n",
    "f2_ = rearrange(f2, \"n d -> 1 d n\").cuda()\n",
    "f3_ = rearrange(f3, \"n d -> 1 d n\").cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0ea672-1032-4df8-a549-92f8d456c05d",
   "metadata": {},
   "source": [
    "## FBNet Cross Transfromer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc9ffd52-c2b7-4180-a6ef-1dc0002c88a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "from FBNet import CrossTransformer as FBCrossTransformer\n",
    "from FBNet import grouping_operation, query_knn\n",
    "from torch import einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "324b7223-7de5-43af-bd3f-ea80cab3c39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FBCrossTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channel,\n",
    "        dim=256,\n",
    "        n_knn=16,\n",
    "        pos_hidden_dim=64,\n",
    "        attn_hidden_multiplier=4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_knn = n_knn\n",
    "\n",
    "        self.pos_mlp = nn.Sequential(\n",
    "            nn.Conv2d(3, pos_hidden_dim, 1),\n",
    "            nn.BatchNorm2d(pos_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(pos_hidden_dim, in_channel, 1),\n",
    "        )\n",
    "\n",
    "        self.attn_mlp = nn.Sequential(\n",
    "            nn.Conv2d(in_channel, dim * attn_hidden_multiplier, 1),\n",
    "            nn.BatchNorm2d(dim * attn_hidden_multiplier),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(dim * attn_hidden_multiplier, in_channel, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, pcd, feat, pcd_feadb, feat_feadb):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pcd: (B, 3, N)\n",
    "            feat: (B, in_channel, N)\n",
    "            pcd_feadb: (B, 3, N2)\n",
    "            feat_feadb: (B, in_channel, N2)\n",
    "\n",
    "        Returns:\n",
    "            Tensor: (B, in_channel, N), shape context feature\n",
    "        \"\"\"\n",
    "        b, _, num_point = pcd.shape\n",
    "\n",
    "        fusion_pcd = torch.cat((pcd, pcd_feadb), dim=2)\n",
    "        fusion_feat = torch.cat((feat, feat_feadb), dim=2)\n",
    "\n",
    "        key_point = pcd\n",
    "        key_feat = feat\n",
    "\n",
    "        # Preception processing between pcd and fusion_pcd\n",
    "        key_point_idx = query_knn(\n",
    "            self.n_knn,\n",
    "            fusion_pcd.transpose(2, 1).contiguous(),\n",
    "            key_point.transpose(2, 1).contiguous(),\n",
    "            include_self=True,\n",
    "        )\n",
    "\n",
    "        group_point = grouping_operation(fusion_pcd, key_point_idx)\n",
    "        group_feat = grouping_operation(fusion_feat, key_point_idx)\n",
    "\n",
    "        # print(f\"{group_point=}\")\n",
    "        # print(f\"{key_point=}\")\n",
    "\n",
    "        qk_rel = key_feat.reshape((b, -1, num_point, 1)) - group_feat\n",
    "        pos_rel = key_point.reshape((b, -1, num_point, 1)) - group_point\n",
    "\n",
    "        pos_embedding = self.pos_mlp(pos_rel)\n",
    "        sample_weight = self.attn_mlp(\n",
    "            qk_rel + pos_embedding\n",
    "        )  # b, in_channel + 3, n, n_knn\n",
    "        sample_weight = torch.softmax(\n",
    "            sample_weight, -1\n",
    "        )  # b, in_channel + 3, n, n_knn\n",
    "\n",
    "        group_feat = group_feat + pos_embedding\n",
    "        refined_feat = einsum(\n",
    "            \"b c i j, b c i j -> b c i\", sample_weight, group_feat\n",
    "        )\n",
    "\n",
    "        return refined_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e6b36fe-eed7-40ed-8105-a27d96d6d2eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 1, 1, 2, 2],\n",
       "        [0, 1, 2, 3, 4, 5]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn(p2, p1, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5afe303-633f-4d2d-84c2-85861b0a47d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 0],\n",
       "         [3, 2],\n",
       "         [5, 4]]], device='cuda:0', dtype=torch.int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_knn(\n",
    "    2,\n",
    "    p2_.transpose(2, 1).contiguous(),\n",
    "    p1_.transpose(2, 1).contiguous(),\n",
    "    include_self=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05b31529-4ca7-48fb-99cd-0cabf2ef777d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_transformer_fb = FBCrossTransformer(\n",
    "    in_channel=2, dim=16, attn_hidden_multiplier=1, pos_hidden_dim=8, n_knn=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff4482bd-6151-4663-9e9c-2e37e74f604b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FBCrossTransformer(\n",
       "  (pos_mlp): Sequential(\n",
       "    (0): Conv2d(3, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(8, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (attn_mlp): Sequential(\n",
       "    (0): Conv2d(2, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(16, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_transformer_fb.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a594a20a-fcca-4690-b732-5ec6954101f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = cross_transformer_fb(p1_, f1_, p2_, f2_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb0a3021-c6be-4c76-8db3-549b9b0f397e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.6808, 1.6808, 1.6808],\n",
       "         [1.5930, 1.5930, 1.5930]]], device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa688db4-68ec-4724-a2d0-4374d235c16e",
   "metadata": {},
   "source": [
    "## My Cross transfomer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a01ae3e4-20fd-4bf4-adda-a62a0d98d847",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossTransformer(MessagePassing):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channel: int,\n",
    "        pos_hidden_dim: int,\n",
    "        attn_hidden_dim: int,\n",
    "        k: int = 2,\n",
    "    ):\n",
    "        super().__init__(aggr=\"add\", flow=\"target_to_source\")\n",
    "        self.k = k\n",
    "        self.pos_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=3, out_features=pos_hidden_dim),\n",
    "            nn.BatchNorm1d(pos_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=pos_hidden_dim, out_features=in_channel),\n",
    "        )\n",
    "\n",
    "        self.attn_mlp = nn.Sequential(\n",
    "            nn.Linear(in_channel, attn_hidden_dim),\n",
    "            nn.BatchNorm1d(attn_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(attn_hidden_dim, in_channel),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self, px, py, fx, fy, edge_index=None, batch_x=None, batch_y=None\n",
    "    ):\n",
    "\n",
    "        # Include self in target point cloud\n",
    "        p_fusion = torch.cat([px, py])\n",
    "        f_fusion = torch.cat([fx, fy])\n",
    "        if batch_x is not None:\n",
    "            fusion_batch = torch.cat([batch_x, batch_y])\n",
    "        else:\n",
    "            fusion_batch = None\n",
    "\n",
    "        if edge_index is None:\n",
    "            edge_index = knn(\n",
    "                x=p_fusion,\n",
    "                y=px,\n",
    "                batch_x=fusion_batch,\n",
    "                batch_y=batch_x,\n",
    "                k=self.k,\n",
    "            )\n",
    "\n",
    "        # flow = \"target_to_source\" => (x_i, x_j), (pos_i, pos_j)\n",
    "        out = self.propagate(edge_index, x=(fx, f_fusion), pos=(px, p_fusion))\n",
    "        return out\n",
    "\n",
    "    def message(self, x_i, x_j, pos_i, pos_j, index):\n",
    "        # Positional embedding\n",
    "        delta_ij = self.pos_mlp(pos_i - pos_j)\n",
    "        # Attention embedding\n",
    "        attn_weights = self.attn_mlp(x_i - x_j + delta_ij)\n",
    "        # Normalize attention\n",
    "        attn_weights = torch_geometric.utils.softmax(\n",
    "            attn_weights, index=index, num_nodes=None\n",
    "        )\n",
    "\n",
    "        # Multiply with the attention weights\n",
    "        out = attn_weights * (x_j + delta_ij)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d845577-7954-4041-b7d5-cb7093256aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_transformer = CrossTransformer(\n",
    "    in_channel=2, pos_hidden_dim=8, attn_hidden_dim=16, k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efb5aa32-1143-4ee4-ab0a-ed1fa21c7933",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'p1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out \u001b[38;5;241m=\u001b[39m cross_transformer(\u001b[43mp1\u001b[49m, p2, f1, f2)\n\u001b[1;32m      2\u001b[0m out, out\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'p1' is not defined"
     ]
    }
   ],
   "source": [
    "out = cross_transformer(p1, p2, f1, f2)\n",
    "out, out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb81856c-4d90-4a59-a66c-ec318bc1e30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tpx = torch.rand(30, 3)\n",
    "tpy = torch.rand(60, 3)\n",
    "tfx = torch.rand(30, 2)\n",
    "tfy = torch.rand(60, 2)\n",
    "x_batch = torch.repeat_interleave(torch.arange(3), 10)\n",
    "y_batch = torch.repeat_interleave(torch.arange(3), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08e31c9e-355f-4d80-974e-2450893dadff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = cross_transformer(tpx, tpy, tfx, tfy, batch_x=x_batch, batch_y=y_batch)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947558a5-acfb-450e-96d6-f7b792729fe1",
   "metadata": {},
   "source": [
    "# Adapt graph pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9d164c-6a8a-4eec-b7d4-228ef0a9b58b",
   "metadata": {},
   "source": [
    "## FB Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0912af2c-1162-4494-9a95-593b2fa68962",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FBNet import (\n",
    "    furthest_point_sample,\n",
    "    gather_operation,\n",
    "    grouping_operation,\n",
    "    query_knn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "364c0e32-afd0-4a80-8b73-1c6eab9b0af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FBAdaptGraphPooling(nn.Module):\n",
    "    def __init__(self, pooling_rate, in_channel, neighbor_num, dim=64):\n",
    "        super().__init__()\n",
    "        self.pooling_rate = pooling_rate\n",
    "        self.neighbor_num = neighbor_num\n",
    "\n",
    "        self.pos_mlp = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Conv2d(64, in_channel, 1),\n",
    "        )\n",
    "\n",
    "        self.attn_mlp = nn.Sequential(\n",
    "            nn.Conv2d(in_channel, dim, 1),\n",
    "            nn.BatchNorm2d(dim),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Conv2d(dim, 3 + in_channel, 1),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        vertices: \"(bs, 3, vertice_num)\",\n",
    "        feature_map: \"(bs, channel_num, vertice_num)\",\n",
    "        idx=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "\n",
    "        Return:\n",
    "            vertices_pool: (bs, 3, pool_vertice_num),\n",
    "            feature_map_pool: (bs, channel_num, pool_vertice_num)\n",
    "        \"\"\"\n",
    "\n",
    "        bs, _, vertice_num = vertices.size()\n",
    "        new_npoints = int(vertice_num * 1.0 / self.pooling_rate + 0.5)\n",
    "        key_points_idx = furthest_point_sample(\n",
    "            vertices.transpose(2, 1).contiguous(), new_npoints\n",
    "        )\n",
    "        key_point = gather_operation(vertices.contiguous(), key_points_idx)\n",
    "        key_feat = gather_operation(feature_map.contiguous(), key_points_idx)\n",
    "\n",
    "        key_point_idx = query_knn(\n",
    "            self.neighbor_num,\n",
    "            vertices.transpose(2, 1).contiguous(),\n",
    "            key_point.transpose(2, 1).contiguous(),\n",
    "            include_self=True,\n",
    "        )\n",
    "\n",
    "        group_point = grouping_operation(vertices.contiguous(), key_point_idx)\n",
    "        group_feat = grouping_operation(feature_map.contiguous(), key_point_idx)\n",
    "\n",
    "        qk_rel = key_feat.reshape((bs, -1, new_npoints, 1)) - group_feat\n",
    "        pos_rel = key_point.reshape((bs, -1, new_npoints, 1)) - group_point\n",
    "\n",
    "        pos_embedding = self.pos_mlp(pos_rel)\n",
    "        sample_weight = self.attn_mlp(\n",
    "            qk_rel + pos_embedding\n",
    "        )  # b, in_channel + 3, n, n_knn\n",
    "        sample_weight = torch.softmax(\n",
    "            sample_weight, -1\n",
    "        )  # b, in_channel + 3, n, n_knn\n",
    "        new_xyz_weight = sample_weight[:, :3, :, :]  # b, 3, n, n_knn\n",
    "        new_feture_weight = sample_weight[\n",
    "            :, 3:, :, :\n",
    "        ]  # b, in_channel, n, n_knn\n",
    "\n",
    "        group_feat = group_feat + pos_embedding  #\n",
    "        new_feat = einsum(\n",
    "            \"b c i j, b c i j -> b c i\", new_feture_weight, group_feat\n",
    "        )\n",
    "        new_point = einsum(\n",
    "            \"b c i j, b c i j -> b c i\", new_xyz_weight, group_point\n",
    "        )\n",
    "\n",
    "        return new_point, new_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "281dbd6e-1073-4b89-8eec-14f1ec9512f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "agp = FBAdaptGraphPooling(in_channel=2, pooling_rate=4, neighbor_num=3).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ddd5645f-3481-4b42-80f0-c3385abefd09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1.3120, 15.3659],\n",
       "          [ 2.6902, 14.9962],\n",
       "          [ 2.8790, 17.4803]]], device='cuda:0', grad_fn=<ViewBackward0>),\n",
       " tensor([[[2.1771, 1.8801],\n",
       "          [2.1400, 1.9103]]], device='cuda:0', grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agp(p2_, f2_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d251023b-2edf-4ce1-a578-f706405dcd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = torch.rand(3, 3, 10).cuda()\n",
    "tf = torch.rand(3, 2, 10).cuda()\n",
    "# x_batch = torch.repeat_interleave(torch.arange(3), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "433531b6-743b-4e20-8613-89e487a5ba9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 3, 3]), torch.Size([3, 2, 3]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1, res2 = agp(tp, tf)\n",
    "res1.shape, res2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368151ac-6172-4b7f-9910-ccb05d698248",
   "metadata": {},
   "source": [
    "## My Adapt Graph Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8827f40b-80c3-4ee2-a4bc-d8c3e5ac3023",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from torch_geometric.nn import fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33c01107-388b-4c99-beb7-0b00f0dd627f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptGraphPooling(MessagePassing):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        pooling_rate: int = 0.25,\n",
    "        pos_hidden_dim: int = 64,\n",
    "        attn_hidden_dim: int = 64,\n",
    "        k=3,\n",
    "    ):\n",
    "        super().__init__(aggr=\"add\", flow=\"target_to_source\")\n",
    "        self.pooling_rate = pooling_rate\n",
    "        self.k = k\n",
    "        self.pos_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=3, out_features=pos_hidden_dim),\n",
    "            nn.BatchNorm1d(pos_hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(in_features=pos_hidden_dim, out_features=in_channels),\n",
    "        )\n",
    "\n",
    "        self.feat_attn_mlp = nn.Sequential(\n",
    "            nn.Linear(in_channels, attn_hidden_dim),\n",
    "            nn.BatchNorm1d(attn_hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(attn_hidden_dim, in_channels),\n",
    "        )\n",
    "\n",
    "        self.pos_attn_mlp = nn.Sequential(\n",
    "            nn.Linear(in_channels, attn_hidden_dim),\n",
    "            nn.BatchNorm1d(attn_hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(attn_hidden_dim, 3),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, pos, batch=None, return_batch=True):\n",
    "\n",
    "        n_points = len(pos)  # careful with batching\n",
    "        idxs = fps(\n",
    "            pos, batch=batch, ratio=self.pooling_rate, random_start=False\n",
    "        )\n",
    "        pos_ = pos[idxs]\n",
    "        x_ = x[idxs]\n",
    "\n",
    "        if batch is not None:\n",
    "            batch_ = batch[idxs]\n",
    "        else:\n",
    "            batch_ = None\n",
    "\n",
    "        edge_index = knn(pos, pos_, batch_x=batch, batch_y=batch_, k=self.k)\n",
    "        out = self.propagate(edge_index, x=(x_, x), pos=(pos_, pos))\n",
    "\n",
    "        new_pos, new_feat = out[:, :3], out[:, 3:]\n",
    "\n",
    "        if return_batch:\n",
    "            return new_pos, new_feat, batch_\n",
    "        else:\n",
    "            return new_pos, new_feat\n",
    "\n",
    "    def message(self, x_i, x_j, pos_i, pos_j, index):\n",
    "        # Positional embedding\n",
    "        delta_ij = self.pos_mlp(pos_i - pos_j)\n",
    "\n",
    "        # Positional weights\n",
    "        pos_weights = self.pos_attn_mlp(x_i - x_j + delta_ij)\n",
    "        pos_weights = torch_geometric.utils.softmax(\n",
    "            pos_weights, index=index, num_nodes=None\n",
    "        )\n",
    "\n",
    "        # Feature weights\n",
    "        feat_weights = self.feat_attn_mlp(x_i - x_j + delta_ij)\n",
    "        feat_weights = torch_geometric.utils.softmax(\n",
    "            feat_weights, index=index, num_nodes=None\n",
    "        )\n",
    "\n",
    "        # Concatenate to return\n",
    "        out = torch.cat(\n",
    "            [pos_weights * pos_j, feat_weights * (x_j + delta_ij)], dim=-1\n",
    "        )\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "44a5eeb0-168c-4f01-97b2-e50eb11df676",
   "metadata": {},
   "outputs": [],
   "source": [
    "agp = AdaptGraphPooling(2, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8c8777e9-e9b7-4228-9ff4-d113a168ad8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np, nf = agp(f2, p2, return_batch=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0cd44c7-8055-4c66-937b-8320c5be6bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = torch.rand(30, 3)\n",
    "tf = torch.rand(30, 2)\n",
    "x_batch = torch.repeat_interleave(torch.arange(3), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2ad5c80-bb13-481d-b3c6-1a7add195900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([9, 3]), torch.Size([9, 2]), tensor([0, 0, 0, 1, 1, 1, 2, 2, 2]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agp = AdaptGraphPooling(2, k=2)\n",
    "res1, res2, res_batch = agp(tf, tp, batch=x_batch)\n",
    "res1.shape, res2.shape, res_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e3e871-9dc1-4ec2-b7d7-13c5d529f4bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bdf4bee9-f9a6-4189-956f-364f0d23d9a7",
   "metadata": {},
   "source": [
    "# HGNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89629de4-a0a2-4043-9908-4775e69b67a8",
   "metadata": {},
   "source": [
    "## FBNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a514156-4108-4d6c-a5a2-73f2cfd12b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0cba8807-83a4-465e-82b6-d5e7bde050a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FBNet import (\n",
    "    furthest_point_sample,\n",
    "    gather_operation,\n",
    "    group_local,\n",
    "    grouping_operation,\n",
    "    query_knn,\n",
    ")\n",
    "from torch import einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "876db8f5-9d93-4b82-a02c-91a201fc0ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FBEdgeConv(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        x: point cloud, [B, C1, N]\n",
    "    Return:\n",
    "        x: point cloud, [B, C2, N]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_channel, output_channel, k):\n",
    "        super().__init__()\n",
    "        self.num_neigh = k\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(2 * input_channel, output_channel // 2, kernel_size=1),\n",
    "            nn.BatchNorm2d(output_channel // 2),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Conv2d(output_channel // 2, output_channel // 2, kernel_size=1),\n",
    "            nn.BatchNorm2d(output_channel // 2),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Conv2d(output_channel // 2, output_channel, kernel_size=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch_size, dims, num_points = inputs.shape\n",
    "        if self.num_neigh is not None:\n",
    "            neigh_feature = group_local(inputs, k=self.num_neigh).contiguous()\n",
    "            central_feat = inputs.unsqueeze(dim=3).repeat(\n",
    "                1, 1, 1, self.num_neigh\n",
    "            )\n",
    "        else:\n",
    "            central_feat = torch.zeros(batch_size, dims, num_points, 1).to(\n",
    "                inputs.device\n",
    "            )\n",
    "            neigh_feature = inputs.unsqueeze(-1)\n",
    "        edge_feature = central_feat - neigh_feature\n",
    "        feature = torch.cat((edge_feature, central_feat), dim=1)\n",
    "        feature = self.conv(feature)\n",
    "        central_feature = feature.max(dim=-1, keepdim=False)[0]\n",
    "        return central_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e16c7250-cb90-4994-9ed5-ed1c80b83085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical Graph-based Network\n",
    "class FBHGNet(nn.Module):\n",
    "    def __init__(self, num_pc=128, g_feat_dim=1024, using_max=True, k=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.using_max = using_max\n",
    "        self.num_pc = num_pc\n",
    "        pool_num = 2048\n",
    "\n",
    "        self.out_channel = g_feat_dim // 2\n",
    "\n",
    "        # HGNet econder\n",
    "        self.gcn_1 = FBEdgeConv(3, 64, k)\n",
    "\n",
    "        self.graph_pooling_1 = FBAdaptGraphPooling(4, 64, k)\n",
    "        self.gcn_2 = FBEdgeConv(64, 128, k)\n",
    "        self.graph_pooling_2 = FBAdaptGraphPooling(2, 128, k)\n",
    "        self.gcn_3 = FBEdgeConv(128, 512, k)\n",
    "\n",
    "        # Fully-connected decoder\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512 * 2, 1024),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Linear(1024, 3 * num_pc),\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        device = inputs.device\n",
    "        batch_size = inputs.size(0)\n",
    "        x1 = self.gcn_1(inputs)\n",
    "        print(x1.shape)\n",
    "\n",
    "        vertices_pool_1, x1 = self.graph_pooling_1(inputs, x1)\n",
    "\n",
    "        # B x 128 x 512\n",
    "        x2 = self.gcn_2(x1)\n",
    "        print(x2.shape)\n",
    "\n",
    "        vertices_pool_2, x2 = self.graph_pooling_2(vertices_pool_1, x2)\n",
    "\n",
    "        # B x 256 x 256\n",
    "        x3 = self.gcn_3(x2)\n",
    "\n",
    "        print(x3.shape)\n",
    "        # Global feature generating B*1024\n",
    "        feat_max = F.adaptive_max_pool1d(x3, 1).view(batch_size, -1)\n",
    "        feat_avg = F.adaptive_avg_pool1d(x3, 1).view(batch_size, -1)\n",
    "        feat_gf = torch.cat((feat_max, feat_avg), dim=1)\n",
    "        print(feat_gf.shape)\n",
    "\n",
    "        # Decoder coarse input\n",
    "        print(self.fc(feat_gf).shape)\n",
    "        coarse_pcd = self.fc(feat_gf).reshape(batch_size, -1, self.num_pc)\n",
    "\n",
    "        return coarse_pcd, feat_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ccda7bea-c3d2-4149-9b5b-cdffc048dab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = FBHGNet(128, k=16).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e7d3d363-4aed-437d-a614-d6b9c40cb0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 256])\n",
      "torch.Size([1, 1024])\n"
     ]
    }
   ],
   "source": [
    "cpcd, fmax = net(p3_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "231d2743-87e5-46f8-b48d-8b8bf142c661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 128]), torch.Size([1, 512]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpcd.shape, fmax.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dec71372-0a3e-4f4a-b880-34d15cfa5fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 64, 10])\n",
      "torch.Size([4, 128, 3])\n",
      "torch.Size([4, 512, 2])\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([4, 99])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 3, 33]), torch.Size([4, 512]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = FBHGNet(33, k=2).cuda()\n",
    "\n",
    "# tp = torch.rand(3, 3, 2048).cuda()\n",
    "tp = torch.rand(4, 3, 10).cuda()\n",
    "res1, res2 = net(tp)\n",
    "res1.shape, res2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49a6e69-7c7f-4932-925d-bda25e9ad603",
   "metadata": {},
   "source": [
    "## My HGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb33ee4f-c15c-482e-9b3f-dd5164f21b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import (\n",
    "    DynamicEdgeConv,\n",
    "    EdgeConv,\n",
    "    global_max_pool,\n",
    "    global_mean_pool,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bee6df96-b066-48a1-98d5-066bc42c4ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_conv_nn(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(2 * in_channels, out_channels // 2),\n",
    "        nn.BatchNorm1d(out_channels // 2),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        nn.Linear(out_channels // 2, out_channels),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0a8b3f1d-cb1e-48da-a72d-37a7f7a8803a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HGNet(nn.Module):\n",
    "    def __init__(self, num_pc: int = 128, k=3):\n",
    "        super().__init__()\n",
    "\n",
    "        # HGNet econder\n",
    "        self.num_pc = num_pc\n",
    "\n",
    "        self.gcn_1 = DynamicEdgeConv(nn=edge_conv_nn(3, 64), k=k)\n",
    "        self.graph_pooling_1 = AdaptGraphPooling(\n",
    "            in_channels=64, pooling_rate=0.25, k=k\n",
    "        )\n",
    "        self.gcn_2 = DynamicEdgeConv(nn=edge_conv_nn(64, 128), k=k)\n",
    "        self.graph_pooling_2 = AdaptGraphPooling(\n",
    "            in_channels=128, pooling_rate=0.5, k=k\n",
    "        )\n",
    "        self.gcn_3 = DynamicEdgeConv(nn=edge_conv_nn(128, 512), k=k)\n",
    "\n",
    "        # Fully-connected decoder\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_features=512 * 2, out_features=1024),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Linear(in_features=1024, out_features=1024),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Linear(in_features=1024, out_features=3 * num_pc),\n",
    "        )\n",
    "\n",
    "    def forward(self, pos, batch=None, return_batch=True):\n",
    "        device = pos.device\n",
    "        x1 = self.gcn_1(pos, batch=batch)\n",
    "        print(x1.shape)\n",
    "\n",
    "        vertices_pool_1, x1, batch = self.graph_pooling_1(x1, pos, batch=batch)\n",
    "\n",
    "        x2 = self.gcn_2(x1, batch=batch)\n",
    "        print(x2.shape)\n",
    "\n",
    "        vertices_pool_2, x2, batch = self.graph_pooling_2(\n",
    "            x2, vertices_pool_1, batch=batch\n",
    "        )\n",
    "\n",
    "        x3 = self.gcn_3(x2, batch=batch)\n",
    "\n",
    "        # Global feature generating B*1024\n",
    "        print(x3.shape)\n",
    "        feat_max = global_max_pool(x3, batch=batch)\n",
    "        feat_avg = global_mean_pool(x3, batch=batch)\n",
    "        feat_gf = torch.cat((feat_max, feat_avg), dim=1)\n",
    "        # Decoder coarse input\n",
    "        print(feat_gf.shape)\n",
    "        coarse_pcd = self.fc(feat_gf)\n",
    "        print(coarse_pcd.shape)\n",
    "\n",
    "        coarse_pcd = rearrange(coarse_pcd, \"b (d n)-> (b n) d\", d = 3)\n",
    "\n",
    "        if return_batch:\n",
    "            batch = torch.repeat_interleave(\n",
    "                torch.arange(feat_max.shape[0]), self.num_pc\n",
    "            ).to(device)\n",
    "            return coarse_pcd, feat_max, batch\n",
    "        else:\n",
    "            return coarse_pcd, feat_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91251631-c433-427e-9d92-6c2b89589a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = HGNet(128, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f7fd60d6-2218-42ce-ba18-464a303ccf1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'p3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m cpcd, feat_max \u001b[38;5;241m=\u001b[39m net(\u001b[43mp3\u001b[49m, return_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'p3' is not defined"
     ]
    }
   ],
   "source": [
    "cpcd, feat_max = net(p3, return_batch=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5243097-fa93-4987-8d41-f685cce70a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpcd.shape, feat_max.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8f2d51be-d675-468f-9a39-3f6d1211016f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = torch.rand(40, 3).cuda()\n",
    "tf = torch.rand(40, 2).cuda()\n",
    "x_batch = torch.repeat_interleave(torch.arange(4), 10).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "276207b0-5b71-4ca8-8776-c4d59b8233e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 64])\n",
      "torch.Size([12, 128])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([4, 99])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([132, 3]), torch.Size([4, 512]), torch.Size([132]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = HGNet(33, k=2).cuda()\n",
    "res1, res2, batch_res = net(tp, batch=x_batch)\n",
    "res1.shape, res2.shape, batch_res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22ee720-e24e-4537-8203-e364f17d6690",
   "metadata": {},
   "source": [
    "# FBAC block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb0a8ac-51bf-479e-ab90-70c81371fdfa",
   "metadata": {},
   "source": [
    "## FBNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "4bc7b477-70dc-4a6d-b142-18a1fbeb9870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FBNet import MLP_CONV\n",
    "from FBNet import CrossTransformer as FBCrossTransformer\n",
    "from FBNet import EdgeConv as FBEdgeConv\n",
    "from FBNet import NodeShuffle as FBNodeShuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "9adfae53-5851-41b1-bcda-5b7fc7b13dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FBAC_BLOCK(nn.Module):\n",
    "    def __init__(self, up_factor=2, cycle_num=1, k=16):\n",
    "        \"\"\"\n",
    "        des: Feedback-Aware Completion block\n",
    "        input: point cloud: B, 3, N\n",
    "        param: up_factor: up-sampling ratio\n",
    "               cycle_num: number of time steps\n",
    "        return: point cloud: B, 3, N * up_factor\n",
    "\n",
    "        \"\"\"\n",
    "        super(FBAC_BLOCK, self).__init__()\n",
    "        # self.cyc_num = cyc_num\n",
    "        self.up_factor = up_factor\n",
    "        # self.gf_mode = gf_mode\n",
    "        # self.weight = weight\n",
    "\n",
    "        self.nodeshuffle = FBNodeShuffle(\n",
    "            128, 128, neighbor_num=8, scale=up_factor\n",
    "        )\n",
    "        self.mlp_delta = MLP_CONV(in_channel=128, layer_dims=[128, 64, 3])\n",
    "\n",
    "        self.ext = FBEdgeConv(3, 128, k)\n",
    "\n",
    "        self.mlp = MLP_CONV(in_channel=128 * 2, layer_dims=[256, 128])\n",
    "\n",
    "        self.fb_exploit = FBCrossTransformer(in_channel=128, dim=64)\n",
    "\n",
    "        self.up_sampler = nn.Upsample(scale_factor=up_factor)\n",
    "\n",
    "        # self.alphas = nn.Embedding(cycle_num,1,_weight=torch.ones(cycle_num,1))\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, pcd, pcd_next, feat_next, cycle=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pcd: Tensor, (B, 3, N_prev)\n",
    "            pcd_next: Tensor, (B, 3, N_next)\n",
    "            K_next: Tensor, (B, 128, N_next)\n",
    "\n",
    "        Returns:\n",
    "            pcd_child: Tensor, up sampled point cloud, (B, 3, N_prev * up_factor)\n",
    "        \"\"\"\n",
    "\n",
    "        b, C, n_prev = pcd.shape\n",
    "\n",
    "        # Step 1: Feature Extraction\n",
    "        feat = self.ext(pcd)\n",
    "        print(feat.shape)\n",
    "        feat = self.mlp(\n",
    "            torch.cat(\n",
    "                [\n",
    "                    feat,\n",
    "                    torch.max(feat, 2, keepdim=True)[0].repeat(\n",
    "                        (1, 1, feat.size(2))\n",
    "                    ),\n",
    "                ],\n",
    "                1,\n",
    "            )\n",
    "        )\n",
    "        print(\n",
    "            torch.cat(\n",
    "                [\n",
    "                    feat,\n",
    "                    torch.max(feat, 2, keepdim=True)[0].repeat(\n",
    "                        (1, 1, feat.size(2))\n",
    "                    ),\n",
    "                ],\n",
    "                1,\n",
    "            ).shape\n",
    "        )\n",
    "\n",
    "        # Step 2: Feedback Exploitation\n",
    "        if pcd_next is None:\n",
    "            pcd_next, feat_next = pcd, feat\n",
    "        feat = self.fb_exploit(pcd, feat, pcd_next, feat_next)\n",
    "\n",
    "        # Step 3: Feature Expansion\n",
    "        feat = self.nodeshuffle(feat)\n",
    "\n",
    "        # Step 4: Coordinate Generation\n",
    "        delta = self.mlp_delta(feat)\n",
    "        pcd_child = self.up_sampler(pcd) + delta\n",
    "\n",
    "        return pcd_child, feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "07743726-0967-43ff-8694-570c2f6964a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fbac_block = FBAC_BLOCK().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "83d11338-7883-46ae-ae72-ecd1d0590bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "f3_ = torch.ones(1, 128, 2048).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "b6c0539d-e264-4d4b-90e0-2d4fefdc1c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 2048])\n",
      "torch.Size([1, 256, 2048])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 4096]), torch.Size([1, 128, 4096]))"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1, r2 = fbac_block(p3_, p3_, f3_)\n",
    "r1.shape, r2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "c1bfc7f9-1d80-43e7-88d3-578398877739",
   "metadata": {},
   "outputs": [],
   "source": [
    "tpx = torch.rand(3, 3, 10).cuda()\n",
    "tpy = torch.rand(3, 3, 20).cuda()\n",
    "tfx = torch.rand(3, 128, 10).cuda()\n",
    "tfy = torch.rand(3, 128, 20).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "0868836a-4cf5-4a3a-b4a2-aed6bab9e203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 128, 10])\n",
      "torch.Size([3, 256, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 3, 20]), torch.Size([3, 128, 20]))"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbac_block = FBAC_BLOCK(k=2).cuda()\n",
    "res1, res2 = fbac_block(tpx, tpy, tfy)\n",
    "res1.shape, res2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fcfde6-ffe7-4ec5-a45f-47dd549db911",
   "metadata": {},
   "source": [
    "## My fbac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed5f19e-4122-4c59-8155-58874321464a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1852a64e-f091-4d45-a86c-cb057af18f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import upsample\n",
    "from einops import rearrange, reduce, repeat\n",
    "from torch_geometric.nn import MLP, DynamicEdgeConv\n",
    "from torch_geometric.utils import to_dense_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "041d465f-2449-4890-8300-fe4724f2e78b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'upsample' from 'G:\\\\Knowledge\\\\Faculta-Trash\\\\Master-ML2022\\\\RTML\\\\3DVision\\\\PointCompletion\\\\FBNet\\\\upsample.py'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "importlib.reload(upsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b457562-88cd-4080-bdc1-19ecaefce53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FbacBlock(nn.Module):\n",
    "    def __init__(self, up_factor: int = 2):\n",
    "        super().__init__()\n",
    "        self.up_factor = up_factor\n",
    "\n",
    "        # Feature extraction\n",
    "        self.gcn = DynamicEdgeConv(nn=edge_conv_nn(3, 128), k=16)\n",
    "        self.mlp = MLP([128 * 2, 256, 128])\n",
    "\n",
    "        # Node expansion\n",
    "        self.nodeshuffle = upsample.NodeShuffle(128, 128, k=8, r=up_factor)\n",
    "\n",
    "        # Coordinate generation\n",
    "        self.mlp_delta = MLP([128, 128, 64, 3])\n",
    "\n",
    "        # Feedback exploitation\n",
    "        self.cross_transformer = CrossTransformer(\n",
    "            in_channel=128, pos_hidden_dim=64, attn_hidden_dim=64\n",
    "        )\n",
    "\n",
    "        self.up_sampler = nn.Upsample(scale_factor=up_factor)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pcd,\n",
    "        pcd_next=None,\n",
    "        feat_next=None,\n",
    "        batch_current=None,\n",
    "        batch_next=None,\n",
    "        return_batch: bool = True,\n",
    "    ):\n",
    "        # b, C, n_prev = pcd.shape\n",
    "\n",
    "        # Step 1: Feature Extraction\n",
    "        feat = self.gcn(pcd, batch=batch_current)\n",
    "        feat = to_dense_batch(feat, batch=batch_current)[0]\n",
    "        feat = torch.cat(\n",
    "            [\n",
    "                feat,\n",
    "                repeat(\n",
    "                    reduce(feat, \"b n c -> b n\", \"max\"),\n",
    "                    \"b n -> b n new_axis\",\n",
    "                    new_axis=feat.shape[-1],\n",
    "                ),\n",
    "            ],\n",
    "            -1,\n",
    "        )\n",
    "        feat = rearrange(feat, \"b n c -> (b n) c\")\n",
    "        feat = self.mlp(feat)\n",
    "\n",
    "        # Step 2: Feedback Exploitation\n",
    "        if pcd_next is None:\n",
    "            pcd_next, feat_next, batch_next = pcd, feat, batch_current\n",
    "        feat = self.cross_transformer(\n",
    "            pcd,\n",
    "            pcd_next,\n",
    "            feat,\n",
    "            feat_next,\n",
    "            batch_x=batch_current,\n",
    "            batch_y=batch_next,\n",
    "        )\n",
    "\n",
    "        # Step 3: Feature Expansion\n",
    "        feat, batch = self.nodeshuffle(\n",
    "            feat, batch=batch_current, return_batch=True\n",
    "        )\n",
    "\n",
    "        # Step 4: Coordinate Generation\n",
    "        delta = self.mlp_delta(feat)\n",
    "        u = repeat(pcd, \"n c -> (n d) c\", d=self.up_factor)\n",
    "        pcd_child = u + delta\n",
    "\n",
    "        if return_batch:\n",
    "            return pcd_child, feat, batch\n",
    "        else:\n",
    "            return pcd_child, feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aab7ba13-d138-4a18-89b7-8cd6e4086526",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.rand(2048, 3)\n",
    "p_next = torch.rand(2048, 3)\n",
    "feat_next = torch.rand(2048, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "41d119f3-f795-4a0a-870a-76430a183c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fbac_block = FbacBlock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "8f7bfa20-e4e1-4f01-8845-2a490f6eb53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f3 = torch.rand(2048, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "0fec5927-2206-4a3b-ab23-5ca22be79f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4096, 3]), torch.Size([4096, 128]))"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o, t = fbac_block(p3, p3, f3, return_batch=False)\n",
    "o.shape, t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "170d62d1-55af-4e6d-a44e-2c0a81c18ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tpx = torch.rand(30, 3)\n",
    "tpy = torch.rand(60, 3)\n",
    "tfx = torch.rand(30, 128)\n",
    "tfy = torch.rand(60, 128)\n",
    "x_batch = torch.repeat_interleave(torch.arange(3), 10)\n",
    "y_batch = torch.repeat_interleave(torch.arange(3), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "10899b00-93fb-4189-91bd-fc49119b630a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([60, 3]), torch.Size([60, 128]), torch.Size([60]))"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbac_block = FbacBlock()\n",
    "res1, res2, batch_res = fbac_block(\n",
    "    tpx, tpy, tfy, batch_current=x_batch, batch_next=y_batch\n",
    ")\n",
    "res1.shape, res2.shape, batch_res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba68808e-92d8-4171-8382-1b53580da04a",
   "metadata": {},
   "source": [
    "# Fbac Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8cd00a8d-18d5-4c19-a25a-0d2fbbe3888b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops.layers.torch import Rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e4a22066-0f71-4c80-8062-35647b9f4f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedbackRefinementNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        up_factors=None,\n",
    "        cycle_num=1,\n",
    "        n_points_start=512,\n",
    "        return_all: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.return_all = return_all\n",
    "        self.n_points_start = n_points_start\n",
    "        self.cycle_num = cycle_num\n",
    "        if up_factors is None:\n",
    "            up_factors = [1]\n",
    "\n",
    "        self.uppers = nn.ModuleList(\n",
    "            [FbacBlock(up_factor=factor) for factor in up_factors]\n",
    "        )\n",
    "\n",
    "        self.flatten_batch = Rearrange(\"b n c -> (b n) c\")\n",
    "\n",
    "    def forward(self, pcd, partial):\n",
    "\n",
    "        # Init input\n",
    "        arr_pcd = []\n",
    "        pcd = fps_subsample(\n",
    "            torch.cat([pcd, partial], dim=1), self.n_points_start\n",
    "        )  # [b n_start 3]\n",
    "\n",
    "        feat_state = []\n",
    "        pcd_state = []\n",
    "\n",
    "        for cycle in range(self.cycle_num):\n",
    "            pcd_list = []\n",
    "            feat_list = []\n",
    "            for upper_idx, upper in enumerate(self.uppers):\n",
    "                # First timestep\n",
    "                if cycle == 0:\n",
    "                    # Add partial and fps only when they're available\n",
    "                    if upper_idx > 0:\n",
    "                        n_points = pcd.shape[1]\n",
    "                        # Concatenate pcd and partial\n",
    "                        pcd = torch.cat([pcd, partial], dim=1)\n",
    "                        # Sample back with fps\n",
    "                        pcd = fps_subsample(pcd, n_points)\n",
    "\n",
    "                    batch_current = get_batch(pcd)\n",
    "                    pcd, feat, b_ = upper(\n",
    "                        self.flatten_batch(pcd), batch_current=batch_current\n",
    "                    )\n",
    "                    pcd = to_dense_batch(pcd, batch=b_)[0]\n",
    "\n",
    "                    print(pcd.shape, feat.shape)\n",
    "                # Next timesteps\n",
    "                else:\n",
    "                    pcd_next = pcd_state[cycle - 1][upper_idx]\n",
    "                    feat_next = feat_state[cycle - 1][upper_idx]\n",
    "\n",
    "                    # First fbac block\n",
    "                    if upper_idx == 0:\n",
    "                        pcd = pcd_state[cycle - 1][0]\n",
    "                        pcd = torch.cat([pcd, partial], dim=1)\n",
    "                        pcd = fps_subsample(pcd, self.n_points_start)\n",
    "                    else:\n",
    "                        pcd = pcd_list[upper_idx - 1]  # take last pcd\n",
    "                        n_points = pcd_state[cycle - 1][upper_idx - 1].shape[1]\n",
    "                        pcd = torch.cat([pcd, partial], dim=1)\n",
    "                        pcd = fps_subsample(pcd, n_points)\n",
    "\n",
    "                    batch_current = get_batch(pcd)\n",
    "                    batch_next = get_batch(pcd_next)\n",
    "                    pcd, feat, b_ = upper(\n",
    "                        self.flatten_batch(pcd),\n",
    "                        self.flatten_batch(pcd_next),\n",
    "                        feat_next,\n",
    "                        batch_current=batch_current,\n",
    "                        batch_next=batch_next,\n",
    "                    )\n",
    "                    pcd = to_dense_batch(pcd, batch=b_)[0]\n",
    "\n",
    "                pcd_list.append(pcd)\n",
    "                feat_list.append(feat)\n",
    "\n",
    "                if self.return_all:\n",
    "                    arr_pcd.append(pcd)\n",
    "                else:\n",
    "                    if cycle == self.cycle_num - 1:\n",
    "                        arr_pcd.append(pcd)\n",
    "\n",
    "            # Saving present time step states\n",
    "            pcd_state.append(pcd_list)\n",
    "            feat_state.append(feat_list)\n",
    "        return arr_pcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c8d4f1d2-07c0-4f98-856f-b8229e07979d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        n_pc = 128\n",
    "        n_points_start = 512\n",
    "        n_points = 2048\n",
    "\n",
    "        self.coarse_net = HGNet(num_pc=n_pc)\n",
    "\n",
    "        up_factors = [1, 2, 2]\n",
    "\n",
    "        cycle_num = 3\n",
    "        self.refiner = FeedbackRefinementNet(\n",
    "            up_factors=up_factors,\n",
    "            cycle_num=cycle_num,\n",
    "            return_all=True,\n",
    "            n_points_start=n_points_start,\n",
    "        )\n",
    "        self.flatten_batch = Rearrange(\"b n c -> (b n) c\")\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Coarse generation\n",
    "        coarse_pcd, _, batch = self.coarse_net(\n",
    "            self.flatten_batch(x), batch=get_batch(x)\n",
    "        )\n",
    "        p\n",
    "        print(coarse_pcd.shape, batch.shape)\n",
    "        # feedback refinement stage\n",
    "        coarse_pcd_dense = to_dense_batch(coarse_pcd, batch=batch)[0]\n",
    "        res_pcds = self.refiner(coarse_pcd_dense, x)\n",
    "\n",
    "        fine = res_pcds[-1]\n",
    "        return fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8681e12a-f6fc-44c6-8a33-ec0188943452",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mG:\\Conda3\\envs\\pytorch-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:749\u001b[0m, in \u001b[0;36mModule.cuda\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    732\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m    733\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[0;32m    734\u001b[0m \n\u001b[0;32m    735\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mG:\\Conda3\\envs\\pytorch-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    640\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 641\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    643\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    644\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    645\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    646\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    651\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    652\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mG:\\Conda3\\envs\\pytorch-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    640\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 641\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    643\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    644\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    645\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    646\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    651\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    652\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping similar frames: Module._apply at line 641 (1 times)]\u001b[0m\n",
      "File \u001b[1;32mG:\\Conda3\\envs\\pytorch-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    640\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 641\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    643\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    644\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    645\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    646\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    651\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    652\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mG:\\Conda3\\envs\\pytorch-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:664\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    660\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    661\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    662\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 664\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    665\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32mG:\\Conda3\\envs\\pytorch-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:749\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    732\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m    733\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[0;32m    734\u001b[0m \n\u001b[0;32m    735\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "model = Model().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "54b68a13-6b48-438b-9cc0-05c53c9105eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "p = torch.rand(3, 2048, 3).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "35fc299f-b6f8-4e22-91f8-2056d68a4d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6144])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_batch(p).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fe9ab69c-0bad-4e8f-b5c1-2629d77ac5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "r1, r2, b = model.coarse_net(model.flatten_batch(p), batch=get_batch(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "08a03ff1-898d-46cc-9352-069df677b4d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([384, 1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "75706236-9b28-4a48-b92d-dc5405c374af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "23fba2cd-94ac-4f86-9a46-538fd1002d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2048, 3])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e03f3472-03ad-4b1d-b1c5-68523a35507f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3b9bbe5-8141-4146-9b45-9b5b2105343d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([384, 1]) torch.Size([128])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (384) must match the existing size (128) at non-singleton dimension 0.  Target sizes: [384].  Tensor sizes: [128]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mG:\\Conda3\\envs\\pytorch-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[16], line 29\u001b[0m, in \u001b[0;36mModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(coarse_pcd\u001b[38;5;241m.\u001b[39mshape, batch\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# feedback refinement stage\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m coarse_pcd_dense \u001b[38;5;241m=\u001b[39m \u001b[43mto_dense_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoarse_pcd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     30\u001b[0m res_pcds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefiner(coarse_pcd_dense, x)\n\u001b[0;32m     32\u001b[0m fine \u001b[38;5;241m=\u001b[39m res_pcds[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mG:\\Conda3\\envs\\pytorch-env\\lib\\site-packages\\torch_geometric\\utils\\to_dense_batch.py:97\u001b[0m, in \u001b[0;36mto_dense_batch\u001b[1;34m(x, batch, fill_value, max_num_nodes, batch_size)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(batch\u001b[38;5;241m.\u001b[39mmax()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 97\u001b[0m num_nodes \u001b[38;5;241m=\u001b[39m \u001b[43mscatter_add\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_ones\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m cum_nodes \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([batch\u001b[38;5;241m.\u001b[39mnew_zeros(\u001b[38;5;241m1\u001b[39m), num_nodes\u001b[38;5;241m.\u001b[39mcumsum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)])\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_num_nodes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mG:\\Conda3\\envs\\pytorch-env\\lib\\site-packages\\torch_scatter\\scatter.py:29\u001b[0m, in \u001b[0;36mscatter_add\u001b[1;34m(src, index, dim, out, dim_size)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscatter_add\u001b[39m(src: torch\u001b[38;5;241m.\u001b[39mTensor, index: torch\u001b[38;5;241m.\u001b[39mTensor, dim: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     27\u001b[0m                 out: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     28\u001b[0m                 dim_size: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m---> 29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscatter_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mG:\\Conda3\\envs\\pytorch-env\\lib\\site-packages\\torch_scatter\\scatter.py:11\u001b[0m, in \u001b[0;36mscatter_sum\u001b[1;34m(src, index, dim, out, dim_size)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscatter_sum\u001b[39m(src: torch\u001b[38;5;241m.\u001b[39mTensor, index: torch\u001b[38;5;241m.\u001b[39mTensor, dim: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m      9\u001b[0m                 out: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     10\u001b[0m                 dim_size: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m---> 11\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[43mbroadcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m         size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[1;32mG:\\Conda3\\envs\\pytorch-env\\lib\\site-packages\\torch_scatter\\utils.py:12\u001b[0m, in \u001b[0;36mbroadcast\u001b[1;34m(src, other, dim)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(src\u001b[38;5;241m.\u001b[39mdim(), other\u001b[38;5;241m.\u001b[39mdim()):\n\u001b[0;32m     11\u001b[0m     src \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m src\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The expanded size of the tensor (384) must match the existing size (128) at non-singleton dimension 0.  Target sizes: [384].  Tensor sizes: [128]"
     ]
    }
   ],
   "source": [
    "out = model(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c63cec60-3e56-42d6-a6cc-b3ffeda5a61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdf0fdb9-9a42-4ce2-94f9-8a3b352eefcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_LAUNCH_BLOCKING=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97858ba3-20e1-436b-bff1-725924f1e03d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
